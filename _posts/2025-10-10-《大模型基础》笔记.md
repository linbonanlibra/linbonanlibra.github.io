---
title: 《大模型基础》阅读笔记
date: 2025-10-10 19:58:00 +0800
categories: [Note]
tags: [AI, note]     # TAG names should always be lowercase
---
> https://github.com/ZJU-LLMs/Foundations-of-LLMs

## 1.4 基于 Transformer 的语言模型 采样方法
 1.4.1 概率最大化
 - 贪心
   在每轮预测中都选择概率最大的词。但当前概率大的词有可能导致后续的词概率都很小
 - 波束搜索
   每轮保留可能性大的 B 个，最后在 M 个集合里找联合概率最大的。不足是概率最大的文本通常是 **最为常见的文本** ，太“废话”
 1.4.2 随机采样
 - top-k 
   在每轮预测中都选取 K 个概率最高的词作为候选词，然后对这些词的概率用 softmax 函数进行归一化，再根据分布采样出本轮的预测的结果。不足是：<font color='#CD2626'>K值固定，采样结果受候选词分布情况影响大，分布方差大，容易选出概率较小、不合常理的词；分布方差小，选出的K个词同质化严重，结果不够丰富</font>
 - top-P 
   设定阈值 p 来对候选集进行选取，也就是针对候选词的概率选择保留。好处是可以避免选出特别离谱的词，还能保留住更多概率相近的词，增加丰富度(*但如果候选词分布方差特别小，同质化的问题还是存在？只不过能确保候选集里不会都是含义相近的词*)
 - temperature 机制(温度) 
   对softmax函数中的自变量做尺度变换，实现调节随机性。即引入一个变量T，使得当 T > 1 时，Temperature 机制会使得候选集中的词的概率差距减小，分布变得更平坦，从而增加随机性；当 0 < T < 1 时，Temperature 机制会使得候选集中的元素的概率差距加大，强者越强，弱者越弱，概率高的候选词会容易被选到，从而随机性变弱(<font color = '#CD2626'>想要开放性结果，将T设置的大于1；想要保守的、弱随机的结果，比如代码生成，将T设置的 0~1)</font>)

# 6 检索增强生成(RAG)
![img](/assets/img/attachments/Pasted image 20250529095351.png)
给定一个自然语言问题(Query)，检索器将问题进行编码，并从知识库(*如维基百科*)中高效检索出与问题相关的文档。然后，将检索到的知识和原始问题一并传递给大语言模型， 大语言模型根据检索到的知识和原始问题生成最终的输出。核心优势在于不需要对大语言模型的内部知识进行更新，便可改善大语言模型的幻觉现象，提高生成质量。

<font color = '#CD2626'>如何优化设计RAG系统?</font>
> 1. 如何优化检索器与大语言模型的协作?
> 	a. 黑盒增强架构
> 	b. 白盒增强架构
> 2. 如何优化检索过程?
> 	a. 知识库构建
> 	b. 查询增强
> 	c. 检索器
> 	d. 检索效率增强
> 	e. 重排优化 
> 3. 如何优化增强过程?
> 	a. 何时增强
> 	b. 何处增强
> 	c. 多次增强
> 	d. 降本增效


## 6.2 RAG 架构分类
检索增强生成(RAG)系统是一个集成了外部知识库、检索器、生成器等多个功能模块的软件系统。检索器和生成器的协作方式对 RAG 性能的影响最为显著。
![img](/assets/img/attachments/Pasted image 20250605161857.png)
如果大模型是闭源的，那就只能利用它的输出结果，无法调整其参数，所以是“黑盒”；如果大模型是开源的，那就可以对其微调，也就相当于“白盒”。
### :one: 无微调
【典型】In-Context RALM
检索器和语言模型训练后就不再更新，在输入的问题或部分句子作为查询从知识库中检索出相关文档，在生成阶段，直接把这些检索到的文档拼接到prompt上下文里。一个 RAG 任务可能涉及多次执行检索和生成。例如，在一个长文本生成任务中，<font color = '#CD2626'>每生成一定量的文本后，模型就可能会执行一次检索</font>，以确保随着话题的发展，后续生成的内容能够持续保持与话题相关。
> ❗️ 检索步长
> 	【含义】模型在生成文本时，每隔多少个词进行一次检索。
> 	【影响】直接影响到模型的响应速度和信息的即时性。较短的检索步长能够提供更为及 时的信息更新
> ❗️ 检索查询长度
> 	【含义】用于检索的文本片段的长度
> 	【影响】通常被设置为语言模型输入中的最后几个词，以确保检索到的 信息与当前的文本生成任务高度相关

### :two: 检索器微调
【典型】REPLUG LSR
用大模型的输出指导检索器的微调。用大语言模型的困惑度分数作为监督信号来微调检索器，使其能更有效地检索出能够显著降低语言模型困惑度的文档。
>两个关键的概率分布，
>❗️ 检索器输出的文档分布。通过余弦相似度衡量检索得到的文档与上下文的相似度，进而转换成概率值
>❗️ 文档对语言模型的贡献分布。语言模型对原始上下文和每一个检索到的文档生成预测，所有输出结果形成一个概率分布
>
>训练(微调)的方式就是基于以上两个概率分布，力图最小化KL 散度损失函数。

> 其实就是确保检索器输出的文档相关性分布与文档对语言模型的贡献分布相一致
{: .prompt-tip }

### :three: 仅微调语言模型
【典型】RETRO
大语言模型根据检索器提供的上下文信息，对自身参数进行微调。
以RETRO为例：
将知识库中的文本切块 
=> 对每个文本块生成嵌入向量 
=> (微调模型时)每当模型生成一段文本块后，就去知识库中检索出与之最相似的嵌入向量
=> 嵌入向量和模型注意力层的输出一起被外部的 Transformer 编码器编码
=> 新的编码向量直接输入给模型的块交叉编码器的键(key)和值(value)，以捕捉外部知识的关键信息

### :four: 检索器和语言模型协同微调
【典型】Atlas
检索器 和语言模型的参数更新同步进行。
以Atlas为例：在预训练和微调阶段使用 KL 散度损失函数来联合训练检索器和语言模型，检索器和语言模型参数同步被更新，检索器学习向语言模型提供最相关的文档，而语言模型则学习如何利用这些文档来改善其对查询的响应

## 6.3 知识检索
### 📖 数据采集及预处理
文本分块 是将长文本分割成较小文本块的过程。文本分块的具体实施流程通常开始于将长文本拆解为较小的语义单元，如句子或段落。随后，这些单元被逐步组合成更大的块，直到达到预设的块大小，构建出独立的文本片段。为了保持语义连贯性，通常还会在相邻的文本片段之间设置一定的重叠区域。
### 📖 知识库增强
【查询生成】利用大语言模型生成与文档内容紧密相关的**伪查询**，用户真实的查询将与这些伪查询匹配。
> 和 FAQ WIKI 的作用类似，进入文档后从目录里寻找和自己问题最接近的 Q
{: .prompt-tip }

【标题生成】利用大语言模型为没有标题的文档生成标题

### 📖 查询增强
对用户查询的语义和内容进行扩展
【同义词改写】原始查询改写成不同表达方式，分别去检索文档，然后把结果合并去重
【多视角分解】针对一个复杂(或抽象、泛泛)的问题，分解成多个不同视角的具体问题，分别进行检索
【查询内容增强】生成与原始查询相关的上下文信息，来丰富查询内容，比如生成查询的背景信息(*多视角分解的一种特殊形式？*)

### 📖 检索器
【判别式检索器】
通过判别模型对查询和文档是否相关打分。
:one: 稀疏检索器
【典型】TF-IDF、BM25
统计文档中特定词项出现的统计特征来对文档进行编码，然后基于此编码计算查询与知识库中的文档的相似度来进行检索
:two: 稠密检索器
用预训练语言模型对文本生成低维、密集的向量表示，通 过计算向量间的相似度进行检索
- 交叉编码类
  将查询和文档拼接在一起，随后利用预训练语言模型作为编码器生成一个向量表示，再通过一个分类器处理这个向量，最终输出一个介于 0 和 1 之间的数值，表示输入的查询和文档之间的相似程度。📌 计算量大，不适合大规模检索阶段，适用于对少量候选文档进行更精确排序的阶段
- 双编码器类
  查询和文档首先各自通过独立的编码器生成各自的向量表示，再对这两个向量之间的相似度进行计算，以评估它们的相关性 📌 可以预先离线计算并存储所有文档的向量表示，在线检索时则可直接进行向量匹配，匹配效率高，但匹配精确度有影响
【生成式检索器】
利用生成模型，生成式检索器直接将知识 库中的文档信息记忆在模型参数中，直接把输入的查询生成文档的标识符。需要训练专用的模型

### 📖 检索效率增强
通过向量数据库来实现检索中的高效向量存储和查询，向量数据库的核心是设计高效的相似度索引算法
#### :one: 基于空间划分
将搜索空间划分为多个区域来实现索引
- 基于树
  通过一系列规则递归地划分空间，形成一种树状结构，每个叶节点代表一个较小区域，区域内的数据点彼此接近。在查询时，算法从树的根节点出发，逐步深入到合适的叶节点，最后在叶节点内部进行数据点的相似度比较，以找到最近的向量
- 基于哈希
  通过哈希函数将向量映射到哈希表的不同桶中，使得相似向量通常位于同一桶内
#### :two: 基于乘积量化
将高维向量空间划分为多个子空间，并在每个子空间中进行聚类得到码本和码字，以此作为构建索引的基础
#### :three: 基于图
📌 邻近图。在索引构建阶段，将数据集中的每个向量表示为图中的一个节点，并根据向量间的距离或相似性建立边的连接。不同的图索引结构主要体现在其独特的赋边策略上。

### 📖 检索结果重排
对检索到的文档进行重新排序
#### :one: 基于交叉编码
【典型】MiniLM-L
利用交叉编码器(Cross-Encoders)来评估文档与查询之间的语义相关性
[MTEB排行榜（for 重排模型）](https://huggingface.co/spaces/mteb/leaderboard)
#### :two: 基于上下文学习
【典型】RankGPT
![img](/assets/img/attachments/Pasted image 20250611110335.png)
通过设计精巧的 Prompt，使用大语言模型来执行重排任务

## 6.4 生成增强
### ❓何时增强
关键：**判断大语言模型是否具有内部知识**。
#### :one: 外部观测法
通过 Prompt 直接询问模型是否具备内部知识，或应用统计方法对是否具备内部知识进行估计
- 询问
  直接询问和反复询问。让模型重复多次回答同一个问题，看结果是否每次都正确、相似。但可能模型会持续给错误答案导致误判
- 观察训练数据
  看模型的训练数据里是否包含目标知识。但一是很多模型不开源、二是数据量太大了
- 构造伪训练数据统计量
  模型都是基于现有数据训练的，因此认为可获取的数据中，某个实体约“流行”，模型训练的时候训练数据中包含该实体的数据也越多，对实体的“记忆”就越深。<font color = '#CD2626'>其实就是虽然不知道闭源模型究竟用了哪些训练数据，但网上特别常见、流行的数据一定是在训练集里的，偏门、冷门的数据训练模型时也同样会难以获取，相应的模型掌握程度的也受限</font>
#### :two: 内部观测法
通过检测模型内部神经元的状态信息来判断模型是否存在内部知识，这种方法需要对模型参数进行侵入式的探测。
分析模型在生成时每一层的隐藏状态变化，如果模型表现出较高的内部不确定性，如注意力分布较为分散、激活值变化较大等，就可能对当前上下文缺乏充分的理解，从而无法做出有把握的预测。
模型的内部知识检索主要发生在中间层的前馈网络中，可以训练分类器进行判别中间层展现出的不同的动态变化，这种方法被称为<font color = '#CD2626'>探针</font>。

### ❓何处增强
![img](/assets/img/attachments/Pasted image 20250611191949.png)
#### :one: 输入端
直接将检索到的外部知识文本与用户查询拼接到 Prompt 中。此方式的重点在于Prompt 设计以及检索到的外部知识的排序。
#### :two: 中间层
将检索到的外部知识转换为向量表示，插入通过交叉注意力融合到模型的隐藏状态中
#### :three: 输出端
利用检索到的外部知识对大语言模型生成的文本进行校准。(*回到了专家模式、知识库模式？*)

### ❓多次增强
复杂或模糊的问题需要多次检索增强。
#### :one: 分解式增强 
🎯 复杂问题
【典型】DEMONSTRATE–SEARCH–PREDICT(DSP)
![img](/assets/img/attachments/Pasted image 20250611200017.png)
=> 通过上下文学习的方法，将复杂问题分解为子问题 
=> 对子问题 进行迭代检索增强 
=> 根据综合外部知识生成最终回答
> 不同领域的复杂问题可能有着不同的分解范式，因此常需要根据具体任务对问题分解方案进行设计
{: .prompt-tip }

#### :two: 渐进式增强
🎯 模糊问题
【典型】TREE OF CLARIFICATIONS(TOC)
![img](/assets/img/attachments/Pasted image 20250611201136.png)
通过递归式检索来引导大语言模型在树状结构中探索给定模糊问题的多种澄清路径。
根据检索到的相关文档和原始问题，生成一系列具体的细化问题，然后针对每个细化问题，再独立进行检索和进一步细化，这样会形成一颗树，叶子结点就是一个有效答案。
> 计算量会随问题复杂度指数级增长
{: .prompt-tip }

### ⛔️ 降本增效
#### :one: 去除冗余文本
对检索出的原始文本的词句进行过滤，选择出部分有益于增强生成的部分。
- Token级（*如 LongLLMLingua*）
  困惑度低的token，携带的新信息少，模型高概率会预测到，属于冗余。
- 子文本级（*如 Fit-RAG*）
  对子文本进行打分，对不必要的子文本成片删除
- 全文本级（*如 PRCA*）
  直接从整个文档中抽取出重要信息，以去除掉冗余信息
#### :two: 复用计算结果
张量缓存。
【典型】RAGCache