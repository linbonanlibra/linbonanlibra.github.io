[ { "title": "《大模型基础》阅读笔记", "url": "/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80-%E7%AC%94%E8%AE%B0/", "categories": "Note", "tags": "AI, note", "date": "2025-10-10 19:58:00 +0800", "content": "https://github.com/ZJU-LLMs/Foundations-of-LLMs 1.4 基于 Transformer 的语言模型 采样方法 1.4.1 概率最大化 贪心 在每轮预测中都选择概率最大的词。但当前概率大的词有可能导致后续的词概率都很小 波束搜索 每轮保留可能性大的 B 个，最后在 M 个集合里找联合概率最大的。不足是概率最大的文本通常是 最为常见的文本 ，太“废话” 1.4.2 随机采样 top-k 在每轮预测中都选取 K 个概率最高的词作为候选词，然后对这些词的概率用 softmax 函数进行归一化，再根据分布采样出本轮的预测的结果。不足是：K值固定，采样结果受候选词分布情况影响大，分布方差大，容易选出概率较小、不合常理的词；分布方差小，选出的K个词同质化严重，结果不够丰富 top-P 设定阈值 p 来对候选集进行选取，也就是针对候选词的概率选择保留。好处是可以避免选出特别离谱的词，还能保留住更多概率相近的词，增加丰富度(但如果候选词分布方差特别小，同质化的问题还是存在？只不过能确保候选集里不会都是含义相近的词) temperature 机制(温度) 对softmax函数中的自变量做尺度变换，实现调节随机性。即引入一个变量T，使得当 T &gt; 1 时，Temperature 机制会使得候选集中的词的概率差距减小，分布变得更平坦，从而增加随机性；当 0 &lt; T &lt; 1 时，Temperature 机制会使得候选集中的元素的概率差距加大，强者越强，弱者越弱，概率高的候选词会容易被选到，从而随机性变弱(想要开放性结果，将T设置的大于1；想要保守的、弱随机的结果，比如代码生成，将T设置的 0~1)) 6 检索增强生成(RAG) 给定一个自然语言问题(Query)，检索器将问题进行编码，并从知识库(如维基百科)中高效检索出与问题相关的文档。然后，将检索到的知识和原始问题一并传递给大语言模型， 大语言模型根据检索到的知识和原始问题生成最终的输出。核心优势在于不需要对大语言模型的内部知识进行更新，便可改善大语言模型的幻觉现象，提高生成质量。 如何优化设计RAG系统? 如何优化检索器与大语言模型的协作? a. 黑盒增强架构 b. 白盒增强架构 如何优化检索过程? a. 知识库构建 b. 查询增强 c. 检索器 d. 检索效率增强 e. 重排优化 如何优化增强过程? a. 何时增强 b. 何处增强 c. 多次增强 d. 降本增效 6.2 RAG 架构分类 检索增强生成(RAG)系统是一个集成了外部知识库、检索器、生成器等多个功能模块的软件系统。检索器和生成器的协作方式对 RAG 性能的影响最为显著。 如果大模型是闭源的，那就只能利用它的输出结果，无法调整其参数，所以是“黑盒”；如果大模型是开源的，那就可以对其微调，也就相当于“白盒”。 :one: 无微调 【典型】In-Context RALM 检索器和语言模型训练后就不再更新，在输入的问题或部分句子作为查询从知识库中检索出相关文档，在生成阶段，直接把这些检索到的文档拼接到prompt上下文里。一个 RAG 任务可能涉及多次执行检索和生成。例如，在一个长文本生成任务中，每生成一定量的文本后，模型就可能会执行一次检索，以确保随着话题的发展，后续生成的内容能够持续保持与话题相关。 ❗️ 检索步长 \t【含义】模型在生成文本时，每隔多少个词进行一次检索。 \t【影响】直接影响到模型的响应速度和信息的即时性。较短的检索步长能够提供更为及 时的信息更新 ❗️ 检索查询长度 \t【含义】用于检索的文本片段的长度 \t【影响】通常被设置为语言模型输入中的最后几个词，以确保检索到的 信息与当前的文本生成任务高度相关 :two: 检索器微调 【典型】REPLUG LSR 用大模型的输出指导检索器的微调。用大语言模型的困惑度分数作为监督信号来微调检索器，使其能更有效地检索出能够显著降低语言模型困惑度的文档。 两个关键的概率分布， ❗️ 检索器输出的文档分布。通过余弦相似度衡量检索得到的文档与上下文的相似度，进而转换成概率值 ❗️ 文档对语言模型的贡献分布。语言模型对原始上下文和每一个检索到的文档生成预测，所有输出结果形成一个概率分布 训练(微调)的方式就是基于以上两个概率分布，力图最小化KL 散度损失函数。 其实就是确保检索器输出的文档相关性分布与文档对语言模型的贡献分布相一致 :three: 仅微调语言模型 【典型】RETRO 大语言模型根据检索器提供的上下文信息，对自身参数进行微调。 以RETRO为例： 将知识库中的文本切块 =&gt; 对每个文本块生成嵌入向量 =&gt; (微调模型时)每当模型生成一段文本块后，就去知识库中检索出与之最相似的嵌入向量 =&gt; 嵌入向量和模型注意力层的输出一起被外部的 Transformer 编码器编码 =&gt; 新的编码向量直接输入给模型的块交叉编码器的键(key)和值(value)，以捕捉外部知识的关键信息 :four: 检索器和语言模型协同微调 【典型】Atlas 检索器 和语言模型的参数更新同步进行。 以Atlas为例：在预训练和微调阶段使用 KL 散度损失函数来联合训练检索器和语言模型，检索器和语言模型参数同步被更新，检索器学习向语言模型提供最相关的文档，而语言模型则学习如何利用这些文档来改善其对查询的响应 6.3 知识检索 📖 数据采集及预处理 文本分块 是将长文本分割成较小文本块的过程。文本分块的具体实施流程通常开始于将长文本拆解为较小的语义单元，如句子或段落。随后，这些单元被逐步组合成更大的块，直到达到预设的块大小，构建出独立的文本片段。为了保持语义连贯性，通常还会在相邻的文本片段之间设置一定的重叠区域。 📖 知识库增强 【查询生成】利用大语言模型生成与文档内容紧密相关的伪查询，用户真实的查询将与这些伪查询匹配。 和 FAQ WIKI 的作用类似，进入文档后从目录里寻找和自己问题最接近的 Q 【标题生成】利用大语言模型为没有标题的文档生成标题 📖 查询增强 对用户查询的语义和内容进行扩展 【同义词改写】原始查询改写成不同表达方式，分别去检索文档，然后把结果合并去重 【多视角分解】针对一个复杂(或抽象、泛泛)的问题，分解成多个不同视角的具体问题，分别进行检索 【查询内容增强】生成与原始查询相关的上下文信息，来丰富查询内容，比如生成查询的背景信息(多视角分解的一种特殊形式？) 📖 检索器 【判别式检索器】 通过判别模型对查询和文档是否相关打分。 :one: 稀疏检索器 【典型】TF-IDF、BM25 统计文档中特定词项出现的统计特征来对文档进行编码，然后基于此编码计算查询与知识库中的文档的相似度来进行检索 :two: 稠密检索器 用预训练语言模型对文本生成低维、密集的向量表示，通 过计算向量间的相似度进行检索 交叉编码类 将查询和文档拼接在一起，随后利用预训练语言模型作为编码器生成一个向量表示，再通过一个分类器处理这个向量，最终输出一个介于 0 和 1 之间的数值，表示输入的查询和文档之间的相似程度。📌 计算量大，不适合大规模检索阶段，适用于对少量候选文档进行更精确排序的阶段 双编码器类 查询和文档首先各自通过独立的编码器生成各自的向量表示，再对这两个向量之间的相似度进行计算，以评估它们的相关性 📌 可以预先离线计算并存储所有文档的向量表示，在线检索时则可直接进行向量匹配，匹配效率高，但匹配精确度有影响 【生成式检索器】 利用生成模型，生成式检索器直接将知识 库中的文档信息记忆在模型参数中，直接把输入的查询生成文档的标识符。需要训练专用的模型 📖 检索效率增强 通过向量数据库来实现检索中的高效向量存储和查询，向量数据库的核心是设计高效的相似度索引算法 :one: 基于空间划分 将搜索空间划分为多个区域来实现索引 基于树 通过一系列规则递归地划分空间，形成一种树状结构，每个叶节点代表一个较小区域，区域内的数据点彼此接近。在查询时，算法从树的根节点出发，逐步深入到合适的叶节点，最后在叶节点内部进行数据点的相似度比较，以找到最近的向量 基于哈希 通过哈希函数将向量映射到哈希表的不同桶中，使得相似向量通常位于同一桶内 :two: 基于乘积量化 将高维向量空间划分为多个子空间，并在每个子空间中进行聚类得到码本和码字，以此作为构建索引的基础 :three: 基于图 📌 邻近图。在索引构建阶段，将数据集中的每个向量表示为图中的一个节点，并根据向量间的距离或相似性建立边的连接。不同的图索引结构主要体现在其独特的赋边策略上。 📖 检索结果重排 对检索到的文档进行重新排序 :one: 基于交叉编码 【典型】MiniLM-L 利用交叉编码器(Cross-Encoders)来评估文档与查询之间的语义相关性 MTEB排行榜（for 重排模型） :two: 基于上下文学习 【典型】RankGPT 通过设计精巧的 Prompt，使用大语言模型来执行重排任务 6.4 生成增强 ❓何时增强 关键：判断大语言模型是否具有内部知识。 :one: 外部观测法 通过 Prompt 直接询问模型是否具备内部知识，或应用统计方法对是否具备内部知识进行估计 询问 直接询问和反复询问。让模型重复多次回答同一个问题，看结果是否每次都正确、相似。但可能模型会持续给错误答案导致误判 观察训练数据 看模型的训练数据里是否包含目标知识。但一是很多模型不开源、二是数据量太大了 构造伪训练数据统计量 模型都是基于现有数据训练的，因此认为可获取的数据中，某个实体约“流行”，模型训练的时候训练数据中包含该实体的数据也越多，对实体的“记忆”就越深。其实就是虽然不知道闭源模型究竟用了哪些训练数据，但网上特别常见、流行的数据一定是在训练集里的，偏门、冷门的数据训练模型时也同样会难以获取，相应的模型掌握程度的也受限 :two: 内部观测法 通过检测模型内部神经元的状态信息来判断模型是否存在内部知识，这种方法需要对模型参数进行侵入式的探测。 分析模型在生成时每一层的隐藏状态变化，如果模型表现出较高的内部不确定性，如注意力分布较为分散、激活值变化较大等，就可能对当前上下文缺乏充分的理解，从而无法做出有把握的预测。 模型的内部知识检索主要发生在中间层的前馈网络中，可以训练分类器进行判别中间层展现出的不同的动态变化，这种方法被称为探针。 ❓何处增强 :one: 输入端 直接将检索到的外部知识文本与用户查询拼接到 Prompt 中。此方式的重点在于Prompt 设计以及检索到的外部知识的排序。 :two: 中间层 将检索到的外部知识转换为向量表示，插入通过交叉注意力融合到模型的隐藏状态中 :three: 输出端 利用检索到的外部知识对大语言模型生成的文本进行校准。(回到了专家模式、知识库模式？) ❓多次增强 复杂或模糊的问题需要多次检索增强。 :one: 分解式增强 🎯 复杂问题 【典型】DEMONSTRATE–SEARCH–PREDICT(DSP) =&gt; 通过上下文学习的方法，将复杂问题分解为子问题 =&gt; 对子问题 进行迭代检索增强 =&gt; 根据综合外部知识生成最终回答 不同领域的复杂问题可能有着不同的分解范式，因此常需要根据具体任务对问题分解方案进行设计 :two: 渐进式增强 🎯 模糊问题 【典型】TREE OF CLARIFICATIONS(TOC) 通过递归式检索来引导大语言模型在树状结构中探索给定模糊问题的多种澄清路径。 根据检索到的相关文档和原始问题，生成一系列具体的细化问题，然后针对每个细化问题，再独立进行检索和进一步细化，这样会形成一颗树，叶子结点就是一个有效答案。 计算量会随问题复杂度指数级增长 ⛔️ 降本增效 :one: 去除冗余文本 对检索出的原始文本的词句进行过滤，选择出部分有益于增强生成的部分。 Token级（如 LongLLMLingua） 困惑度低的token，携带的新信息少，模型高概率会预测到，属于冗余。 子文本级（如 Fit-RAG） 对子文本进行打分，对不必要的子文本成片删除 全文本级（如 PRCA） 直接从整个文档中抽取出重要信息，以去除掉冗余信息 :two: 复用计算结果 张量缓存。 【典型】RAGCache" }, { "title": "模型服务器内存持续下降问题排查", "url": "/posts/%E5%AE%B9%E5%99%A8%E5%86%85%E5%AD%98%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BF/", "categories": "Note", "tags": "Memory, note", "date": "2022-05-12 18:06:00 +0800", "content": "尝试用 valgrind 分析内存 考虑先尝试定位出 Native 内存到底被什么占用了、是什么申请了内存却没及时释放，最初想用 valgrind 抓一下内存的分配/释放动作，来分析是什么被遗留在了内存里没释放。 但在机器上安装好环境后发现，它要有一个可执行文件才比较便于分析，对于 jvm，它不太好使用。但脱离了jvm，单独把模型加载代码抽出来又感觉不能很好的还原场景。 yum install valgrind valgrind --leak-check=full --show-leak-kinds=all --suppressions=suppression_file --log-file=valgrind_with_suppression.log -v java &lt;Java Class&gt; valgrind --leak-check=full --show-leak-kinds=all -v java 尝试用 jemalloc 分析内存 另一个思路是，尝试将内存分配管理交给jemalloc来做，因为它自带一个prof工具，可以分析内存情况。 参考 https://chenhm.com/post/2018-12-05-debuging-java-memory-leak 和 20210409 kms server内存占用高 在测试机器上安装环境，尝试如下配置后重启机器 export LD_PRELOAD=/usr/lib64/libjemalloc.so export MALLOC_CONF=prof:true,lg_prof_sample:1,lg_prof_interval:26,prof_prefix:/opt/logs/com.sankuai.qcs.service.modelserver/heap/jeprof.out ./jeprof --show_bytes -svg /usr/local/java/bin/java /jeprof.*.heap &gt; ms-jeprof.svg 启动时的 heap dump prof 图如下，没看到明显的来自jvm的大占用，反倒是启动后直接把 mem.free.percent 打到 20%了，触发多次模型重载后甚至导致机器 OOM 里怀疑是 prof 的参数配置的有问题，导致监控成本过高，损害了系统运行(yum install 时是无法开启prof的，要靠源码手动编译时增加参数来开启prof能力，因此怀疑这个功能对性能影响会比较大？)。 虽然尝试变更了几次 prof参数都没能抓到理想的 dump 数据，但在了解 jemalloc 时有了一个新思路，会不会是 glibc 的内存管理特点，导致了内存没被及时还给系统？ glibc 内存碎片 查了一些关于 glibc 内存分配的介绍，越发怀疑可能是内存碎片的问题。 进程向 OS 申请和释放地址空间的接口 sbrk/mmap/munmap 都是系统调用，频繁调用系统调用都比较消耗系统资源的。而且， mmap 申请的内存被 munmap 后，从新申请会产生更多的缺页中断。例如使用 mmap 分配 1M 空间，第一次调用产生了大量缺页中断 (1M/4K 次 ) ，当munmap 后再次分配 1M 空间，会再次产生大量缺页中断。缺页中断是内核行为，会致使内核态CPU消耗较大。另外，若是使用 mmap 分配小内存，会致使地址空间的分片更多，内核的管理负担更大。 同时堆是一个连续空间，而且堆内碎片因为没有归还 OS ，若是可重用碎片，再次访问该内存极可能不需产生任何系统调用和缺页中断，这将大大下降 CPU 的消耗。 所以， glibc 的 malloc 实现中，充分考虑了 sbrk 和 mmap 行为上的差别及优缺点，默认分配大块内存 (128k) 才使用 mmap 得到地址空间，也可经过 mallopt(M_MMAP_THRESHOLD, ) 来修改这个临界值。 还在 openjdk 里翻到了一个关于「通过 jcmd命令主动触发trim内存」(收缩内存？)的讨论 https://bugs.openjdk.java.net/browse/JDK-8269345 看了下他们最终对该指令的代码实现 https://github.com/openjdk/jdk17u/commit/d93500168cd120165fedb9609fdf2e10458976dd?diff=split, 可以看到命令的实现方法在下图，而这个方法里，除去执行前后的信息统计，真正的逻辑只有一句，malloc_trim(0)。 这是 Glibc 的一个方法 https://github.com/lattera/glibc/blob/master/malloc/malloc.h 在测试机器上先重复加载模型，让 mem.free.percent 出现明显下跌，然后登陆机器，切换到 root 权限后 执行 sudo gdb -p 5331 -batch -ex ‘call malloc_trim(0)’ ，通过gdb来调用一次 malloc_trim 函数。 反复几次，可以看到每次执行trim函数后，内存都会明显得到释放。至此，大概率确定是 glibc 内存分配策略下，内存碎片导致的问题 临时解决方案 考虑通过 jni 的方式实现在 jvm 内主动调用 malloc_trim 函数实现对C库缓存的内存的回收。由于我们除了调用该函数外并不需要增加其他自定义的逻辑，因此可以考虑直接利用 JNA 实现。 glic 2.7 malloc.c 源码 https://elixir.bootlin.com/glibc/glibc-2.17/source/malloc/malloc.c glic 2.7 malloc.h https://github.com/apc-llc/glibc-2.17/blob/master/malloc/malloc.h jna 源码 https://github.com/java-native-access/jna jna 示例 https://www.eshayne.com/jnaex/index.html?example=4 import com.sun.jna.Library; import com.sun.jna.Native; import com.sun.jna.Structure; public interface GLibcLibrary extends Library { static GLibcLibrary INSTANCE = Native.loadLibrary(\"libc.so.6\", GLibcLibrary.class); /** @see https://elixir.bootlin.com/glibc/glibc-2.17/source/malloc/malloc.c malloc_trim(size_t pad); If possible, gives memory back to the system (via negative arguments to sbrk) if there is unused memory at the `high' end of the malloc pool. You can call this after freeing large blocks of memory to potentially reduce the system-level memory requirements of a program. However, it cannot guarantee to reduce memory. Under some allocation patterns, some large free blocks of memory will be locked between two used chunks, so they cannot be given back to the system. The `pad' argument to malloc_trim represents the amount of free trailing space to leave untrimmed. If this argument is zero, only the minimum amount of memory to maintain internal data structures will be left (one page or less). Non-zero arguments can be supplied to maintain enough trailing space to service future expected allocations without having to re-obtain memory from the system. Malloc_trim returns 1 if it actually released any memory, else 0. On systems that do not support \"negative sbrks\", it will always return 0. */ int malloc_trim(int size); /** * @see https://elixir.bootlin.com/glibc/glibc-2.17/source/malloc/malloc.c * struct mallinfo __libc_mallinfo(void); * * Returns (by copy) a struct containing various summary statistics */ MallInfo.ByValue mallinfo(); } /** * 是 glibc malloc.h 中定义的 mallinfo struct 映射的 java 结构 */ public class MallInfo extends Structure { public static class ByValue extends MallInfo implements Structure.ByValue { public ByValue() { } public String stats() { return \"===========================================================================\\n\" + \"Arena Total Size: \" + arena / 1024 + \" Kib\\n\" + \"Arena Used Size: \" + uordblks / 1024 + \" Kib\\n\" + \"Arena Free Size: \" + fordblks / 1024 + \" Kib\\n\" + \"Arena Reclaimable Size: \" + keepcost / 1024 + \" Kib\\n\" + \"Arena Free Blocks Count: \" + ordblks + \"\\n\" + \"FastBin Blocks Count: \" + smblks + \"\\n\" + \"FastBin Blocks Total Size : \" + fsmblks / 1024 + \"\\n\" + \"Mapped Blocks Count: \" + hblks + \"\\n\" + \"Mapped Blocks Total Size: \" + hblkhd / 1024 + \" Kib\\n\" + \"Total C Heap Size: \" + (arena / 1024 + hblkhd / 1024) + \" Kib\\n\" + \"注: 因历史原因, mallinfo 中的指标都是int类型, 可能因数值越界而不不准确, 仅供参考\"+ \"===========================================================================\"; } } // current total non-mmapped bytes allocated from system public int arena; // the number of free chunks public int ordblks; /* number of free chunks */ // the number of fastbin blocks (i.e., small chunks that have been freed but not use resused or consolidated) public int smblks; // current number of mmapped regions public int hblks; // total bytes held in mmapped regions public int hblkhd; // the maximum total allocated space. This will be greater than current total if trimming has occurred. public int usmblks; // total bytes held in fastbin blocks public int fsmblks; // current total allocated space (normal or mmapped) public int uordblks; // total free space public int fordblks; // the maximum number of bytes that could ideally be released // back to system via malloc_trim. (\"ideally\" means that // it ignores page restrictions etc.) public int keepcost; public MallInfo() { } @Override protected List&lt;String&gt; getFieldOrder() { return Lists.newArrayList(\"arena\", \"ordblks\", \"smblks\", \"hblks\", \"hblkhd\", \"usmblks\", \"fsmblks\", \"uordblks\", \"fordblks\", \"keepcost\"); } }" }, { "title": "Text and Typography", "url": "/posts/text-and-typography/", "categories": "Blogging, Demo", "tags": "typography", "date": "2019-08-08 11:33:00 +0800", "content": "spaceship 表格样例 这是一个样例 Stage Direct Products ATP Yields Glycolysis 2 ATP |   ^^ 2 NADH 3–5 ATP Pyruvaye oxidation 2 NADH 5 ATP Citric acid cycle 2 ATP |   ^^ 6 NADH 15 ATP ^^ 2 FADH 3 ATP 30–32 ATP ||     Headings H1 — heading H2 — heading H3 — heading H4 — heading Paragraph Quisque egestas convallis ipsum, ut sollicitudin risus tincidunt a. Maecenas interdum malesuada egestas. Duis consectetur porta risus, sit amet vulputate urna facilisis ac. Phasellus semper dui non purus ultrices sodales. Aliquam ante lorem, ornare a feugiat ac, finibus nec mauris. Vivamus ut tristique nisi. Sed vel leo vulputate, efficitur risus non, posuere mi. Nullam tincidunt bibendum rutrum. Proin commodo ornare sapien. Vivamus interdum diam sed sapien blandit, sit amet aliquam risus mattis. Nullam arcu turpis, mollis quis laoreet at, placerat id nibh. Suspendisse venenatis eros eros. Lists Ordered list Firstly Secondly Thirdly Unordered list Chapter Section Paragraph ToDo list Job Step 1 Step 2 Step 3 Description list Sun the star around which the earth orbits Moon the natural satellite of the earth, visible by reflected light from the sun Block Quote This line shows the block quote." } ]
